urgent:
    - find out why double precision plans give big errors for N >= 2048
      (probably, will have to compare kernel output for single and double precision)
    - measure performance for double precision

global:
    - check work for big arrays and batch sizes (and for array size == 1 maybe, just in case)
    - check currently not working testcases (they were added in order to get 100% coverage on initially
      ported code; they do not work because of some bug in PyCuda)
    - find out, why for (2048,) Cuda creates kernels for block size 256, and OpenCL - for 128
    - add 2D/3D tiled batch support (i.e., currently batched FFT supports only arrays, located
      successively in memory; but it is a common use case to transform several tiles of big 2D/3D array in one pass)
    - add support for non-power-of-2 sized arrays

optimizations:
    - replace mad24() with vector function
      (tried: decreases performance; compiler seems to know better when to insert mad's)
    - replace indexes with pointer arithmetic if someone answers in thread
    - recompile kernels in order to increase occupancy
      (tried it; first - it does not increase speed; second - FFT building code starts to produce buggy kernels
      for low block sizes)
